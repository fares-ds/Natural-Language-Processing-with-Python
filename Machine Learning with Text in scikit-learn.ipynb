{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: # NLP (Natural Language Processing) with Python\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Representing text as numerical data\n",
    "2. Reading a text-based dataset into pandas\n",
    "3. Vectorizing our dataset\n",
    "4. Building and evaluating a model\n",
    "5. Comparing models\n",
    "6. Examining a model for further insight\n",
    "7. Practicing this workflow on another dataset\n",
    "8. Tuning the vectorizer (discussion)\n",
    "\n",
    "\n",
    "In this notebook we will discuss a higher level overview of the basics of Natural Language Processing, which basically consists of combining machine learning techniques with text, and using math and statistics to get that text in a format that the machine learning algorithms can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Representing text as numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model training (SMS messages)\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'Please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "type(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# examine the sparse matrix contents\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n",
    "\n",
    "> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- `vect.fit(train)` **learns the vocabulary** of the training data\n",
    "\n",
    "- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n",
    "\n",
    "- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading a text-based dataset into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file into pandas using a relative path\n",
    "# sms = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1', names=['label', 'message'])\n",
    "# sms.dropna(how=\"any\", inplace=True, axis=1)\n",
    "sms = pd.read_csv('data/sms.tsv', delimiter='\\t', names=['label', 'message'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have `4825` ham message and `747` spam message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of feature engineering. The better your domain knowledge on the data, the better your ability to engineer more features from it. Feature engineering is a very large part of spam detection in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len  \n",
       "0          111  \n",
       "1           29  \n",
       "2          155  \n",
       "3           49  \n",
       "4           61  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['message_len'] = sms.message.apply(len)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Message Length')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sms[sms.label=='ham'].message_len.plot(bins=35, kind='hist', color='blue', \n",
    "                                       label='Ham messages', alpha=0.6)\n",
    "sms[sms.label=='spam'].message_len.plot(kind='hist', color='red', \n",
    "                                       label='Spam messages', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Message Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting! Through just basic EDA we've been able to discover a trend that spam messages tend to have more characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4825.0</td>\n",
       "      <td>4825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.482902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.442635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>910.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_num  message_len\n",
       "count     4825.0  4825.000000\n",
       "mean         0.0    71.482902\n",
       "std          0.0    58.442635\n",
       "min          0.0     2.000000\n",
       "25%          0.0    33.000000\n",
       "50%          0.0    52.000000\n",
       "75%          0.0    93.000000\n",
       "max          0.0   910.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms.label=='ham'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>747.0</td>\n",
       "      <td>747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.670683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.873603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>149.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>223.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_num  message_len\n",
       "count      747.0   747.000000\n",
       "mean         1.0   138.670683\n",
       "std          0.0    28.873603\n",
       "min          1.0    13.000000\n",
       "25%          1.0   133.000000\n",
       "50%          1.0   149.000000\n",
       "75%          1.0   157.000000\n",
       "max          1.0   223.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms.label=='spam'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! 910 characters, let's use masking to find this message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms.message_len == 910].message.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Pre-processing\n",
    "\n",
    "Our main issue with our data is that it is all in text format (strings). The classification algorithms that we usally use need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the `bag-of-words` approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "\n",
    "In this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n",
    "\n",
    "As a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the `NLTK` library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here.\n",
    "\n",
    "Let's create a function that will process the string in the message column, then we can just use **apply()** in pandas do process all the text in the DataFrame.\n",
    "\n",
    "First removing punctuation. We can just take advantage of Python's built-in **string** library to get a quick list of all the possible punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len  \n",
       "0          111  \n",
       "1           29  \n",
       "2          155  \n",
       "3           49  \n",
       "4           61  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's \"tokenize\" these messages. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens (words that we actually want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['clean_msg'] = sms.message.apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "      <th>clean_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>Ok lar Joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>Free entry wkly comp win FA Cup final tkts 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>dun say early hor c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len                                          clean_msg  \n",
       "0          111  Go jurong point crazy Available bugis n great ...  \n",
       "1           29                              Ok lar Joking wif oni  \n",
       "2          155  Free entry wkly comp win FA Cup final tkts 21s...  \n",
       "3           49                    dun say early hor c already say  \n",
       "4           61             Nah think goes usf lives around though  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 303), ('ltgt', 276), ('ok', 273), ('go', 250), ('ill', 238), ('got', 232), ('know', 232), ('like', 231), ('call', 231), ('come', 227), ('good', 224), ('love', 190), ('time', 189), ('day', 188), ('going', 167), ('want', 164), ('one', 163), ('home', 160), ('lor', 160), ('need', 157), ('sorry', 153), ('still', 147), ('see', 138), ('n', 137), ('later', 134), ('da', 131), ('r', 131), ('back', 129), ('think', 128), ('well', 127), ('today', 125), ('send', 123), ('tell', 121), ('cant', 119), ('hi', 117), ('take', 112), ('oh', 112), ('much', 112), ('night', 109), ('happy', 106), ('hey', 106), ('great', 100), ('way', 100), ('hope', 100), ('pls', 98), ('work', 97), ('wat', 95), ('thats', 95), ('give', 94), ('dear', 94)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = sms[sms.label=='ham'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
    "ham_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    ham_words.update(msg)\n",
    "    \n",
    "print(ham_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('call', 347), ('free', 216), ('txt', 150), ('mobile', 123), ('text', 120), ('stop', 115), ('claim', 113), ('reply', 101), ('prize', 92), ('get', 83), ('new', 69), ('send', 68), ('nokia', 65), ('urgent', 63), ('cash', 62), ('win', 60), ('contact', 56), ('service', 55), ('please', 52), ('16', 51), ('guaranteed', 50), ('customer', 49), ('week', 49), ('tone', 48), ('per', 46), ('phone', 45), ('18', 43), ('chat', 42), ('awarded', 38), ('draw', 38), ('latest', 36), ('£1000', 35), ('line', 35), ('150ppm', 34), ('mins', 34), ('receive', 33), ('camera', 33), ('1', 33), ('box', 33), ('every', 33), ('message', 32), ('po', 32), ('holiday', 32), ('landline', 32), ('shows', 31), ('£2000', 31), ('go', 31), ('number', 30), ('apply', 29), ('code', 29)]\n"
     ]
    }
   ],
   "source": [
    "words = sms[sms.label=='spam'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
    "spam_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    spam_words.update(msg)\n",
    "    \n",
    "print(spam_words.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Currently, we have the messages as lists of tokens (also known as [lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n",
    "\n",
    "Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "\n",
    "We'll do that in three steps using the bag-of-words model:\n",
    "\n",
    "1. Count how many times does a word occur in each message (Known as term frequency)\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "Let's begin the first step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector will have as many dimensions as there are unique words in the SMS corpus.  We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message. \n",
    "\n",
    "For example:\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Message 1</th> <th>Message 2</th> <th>...</th> <th>Message N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word N Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a [Sparse Matrix](https://en.wikipedia.org/wiki/Sparse_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms.clean_msg\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n",
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the **analyzer** to be our own previously defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x32651 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 66380 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x32651 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14084 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x32651 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 66380 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(X_train_dtm)\n",
    "tfidf_transformer.transform(X_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building and evaluating a model\n",
    "\n",
    "We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899497487437185"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1204,    4],\n",
       "       [  10,  175]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4419                              get free call\n",
       "2652              Text get call phones problems\n",
       "694     purchase stuff today mail po box number\n",
       "2289                              message offer\n",
       "Name: clean_msg, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for false positives (ham incorrectly classifier)\n",
    "# X_test[(y_pred_class==1) & (y_test==0)]\n",
    "X_test[y_pred_class > y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881     Reminder downloaded content already paid Goto ...\n",
       "3530    Xmas New Years Eve tickets sale club day 10am ...\n",
       "1875         Would like see XXX pics hot nearly banned uk\n",
       "1893    CALL 09090900040 LISTEN EXTREME DIRTY LIVE CHA...\n",
       "4298    thesmszonecom lets send free anonymous masked ...\n",
       "4949    Hi Amy sending free phone number couple days g...\n",
       "2821    INTERFLORA Its late order Interflora flowers ...\n",
       "191     unique enough Find 30th August wwwareyouunique...\n",
       "2247    Hi ya babe x 4goten bout scammers getting smar...\n",
       "4514                     Money wining number 946 wot next\n",
       "Name: clean_msg, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for false negatives (spam incorrectly classifier)\n",
    "X_test[y_pred_class < y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Amy sending free phone number couple days give access adult parties'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of false negative \n",
    "X_test[4949]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.70258224e-03, 2.20345006e-05, 5.15497023e-02, ...,\n",
       "       3.11988476e-12, 9.74601697e-01, 1.30546092e-10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848756040809021"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('model',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('bow', CountVectorizer(ngram_range=(1, 2))),  \n",
    "                 ('model', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899497487437185"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1204,    4],\n",
       "       [  10,  175]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparing models\n",
    "\n",
    "We will compare multinomial Naive Bayes with [logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression):\n",
    "\n",
    "> Logistic regression, despite its name, is a **linear model for classification** rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import an instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01291862, 0.00653379, 0.0148246 , ..., 0.01418539, 0.30840057,\n",
       "       0.00469893])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791816223977028"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9940263110792912"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Examining a model for further insight\n",
    "\n",
    "We will examine the our **trained Naive Bayes model** to calculate the approximate **\"spamminess\" of each token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32651"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary of X_train\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['008704050406', '008704050406 sp', '0121', '0121 2025050', '01223585236', '01223585236 xx', '01223585334', '01223585334 cum', '0125698789', '0125698789 ring', '02', '02 user', '020603', '020603 2nd', '0207', '0207 153', '02070836089', '02072069400', '02072069400 bx', '02073162414', '02073162414 costs', '02085076972', '02085076972 reply', '020903', '020903 2nd', '021', '021 3680', '021 3680offer', '050703', '050703 tcsbcm4235wc1n3xx', '0578', '06', '06 good', '061104', '07008009200', '07090201529', '07090298926', '07090298926 reschedule', '071104', '07123456789', '07123456789 87077', '07732584351', '07732584351 rodger', '07734396839', '07734396839 ibh', '07742676969', '07742676969 shows', '0776xxxxxxx', '0776xxxxxxx uve', '07786200117']\n"
     ]
    }
   ],
   "source": [
    "# examine the first 50 tokens\n",
    "print(X_train_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yup go', 'yup going', 'yup havent', 'yup hey', 'yup ive', 'yup izzit', 'yup lunch', 'yup msg', 'yup need', 'yup next', 'yup ok', 'yup shd', 'yup still', 'yup studying', 'yup thk', 'yup trying', 'yup wun', 'ywhere', 'ywhere dogbreath', 'zac', 'zac doesnt', 'zahers', 'zahers got', 'zealand', 'zebra', 'zebra animation', 'zed', 'zed 08701417012', 'zed 08701417012150p', 'zed pobox', 'zeros', 'zeros savings', 'zhong', 'zhong se', 'zindgi', 'zindgi wo', 'zoe', 'zoe 18', 'zoe hit', 'zoom', 'zoom cine', 'zouk', 'zouk nichols', 'zyada', 'zyada kisi', 'üll', 'üll submitting', 'üll take', '〨ud', '〨ud evening']\n"
     ]
    }
   ],
   "source": [
    "# examine the last 50 tokens\n",
    "print(X_train_tokens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 1., 1.],\n",
       "       [2., 2., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32651)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows represent classes, columns represent tokens\n",
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each tokens appears across all HAM meassages\n",
    "ham_token_count = nb.feature_count_[0, :]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each tokens appears across all SPAM meassages\n",
    "spam_token_count = nb.feature_count_[1, :]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008704050406 sp</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121 2025050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham  spam\n",
       "token                     \n",
       "008704050406     0.0   2.0\n",
       "008704050406 sp  0.0   2.0\n",
       "0121             0.0   1.0\n",
       "0121 2025050     0.0   1.0\n",
       "01223585236      0.0   1.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, \n",
    "                       'spam':spam_token_count}).set_index('token')\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cos later</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151 pause</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07815296484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storming msg</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham  spam\n",
       "token                  \n",
       "cos later     1.0   0.0\n",
       "connections   1.0   0.0\n",
       "151 pause     0.0   1.0\n",
       "07815296484   0.0   1.0\n",
       "storming msg  2.0   0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine 5 random DataFrame rows\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3617.,  562.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class \n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can calculate the \"spamminess\" of each token, we need to avoid **dividing by zero** and account for the **class imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cos later</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151 pause</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07815296484</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storming msg</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham  spam\n",
       "token                  \n",
       "cos later     2.0   1.0\n",
       "connections   2.0   1.0\n",
       "151 pause     1.0   2.0\n",
       "07815296484   1.0   2.0\n",
       "storming msg  3.0   1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "tokens['ham'] = tokens.ham + 1\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cos later</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151 pause</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07815296484</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storming msg</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ham      spam\n",
       "token                           \n",
       "cos later     0.000553  0.001779\n",
       "connections   0.000553  0.001779\n",
       "151 pause     0.000276  0.003559\n",
       "07815296484   0.000276  0.003559\n",
       "storming msg  0.000829  0.001779"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the ham and spam counts into frequencies\n",
    "tokens['ham'] = tokens.ham / nb.class_count_[0]\n",
    "tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cos later</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>3.217972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>3.217972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151 pause</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>12.871886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07815296484</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>12.871886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storming msg</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>2.145314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ham      spam  spam_ratio\n",
       "token                                       \n",
       "cos later     0.000553  0.001779    3.217972\n",
       "connections   0.000553  0.001779    3.217972\n",
       "151 pause     0.000276  0.003559   12.871886\n",
       "07815296484   0.000276  0.003559   12.871886\n",
       "storming msg  0.000829  0.001779    2.145314"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the ratio of spam-to-ham for each token\n",
    "tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.158363</td>\n",
       "      <td>572.798932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.133452</td>\n",
       "      <td>482.695730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.076512</td>\n",
       "      <td>276.745552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.071174</td>\n",
       "      <td>257.437722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.056940</td>\n",
       "      <td>205.950178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.056940</td>\n",
       "      <td>205.950178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awarded</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.053381</td>\n",
       "      <td>193.078292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.053381</td>\n",
       "      <td>193.078292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.051601</td>\n",
       "      <td>186.642349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150ppm</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.051601</td>\n",
       "      <td>186.642349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>180.206406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urgent</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.085409</td>\n",
       "      <td>154.462633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mob</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>148.026690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed call</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>148.026690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>po box</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>148.026690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>141.590747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000 cash</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>141.590747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize guaranteed</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>141.590747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tcs</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>141.590747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>135.154804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ringtone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>135.154804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>128.718861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>128.718861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekly</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>122.282918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tscs</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land line</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.064057</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>0.011612</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.153237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ya</th>\n",
       "      <td>0.011888</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.149673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gonna</th>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.146271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.012441</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.143021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh</th>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.143021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>told</th>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.131346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gud</th>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.131346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.013824</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.128719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.123768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>0.057783</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.123176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let</th>\n",
       "      <td>0.014653</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.121433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos</th>\n",
       "      <td>0.014929</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.119184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>something</th>\n",
       "      <td>0.014929</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.119184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.117017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.112911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>0.016588</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.107266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>didnt</th>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.102158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.102158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.102158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.100562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.091942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.091942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.090647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thats</th>\n",
       "      <td>0.020735</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.085813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.048106</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.073976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ill</th>\n",
       "      <td>0.049212</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.072314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.029030</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.061295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>0.030412</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.058509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>0.032347</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.055008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltgt</th>\n",
       "      <td>0.056953</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.031242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32651 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham      spam  spam_ratio\n",
       "token                                     \n",
       "claim       0.000276  0.158363  572.798932\n",
       "prize       0.000276  0.133452  482.695730\n",
       "guaranteed  0.000276  0.076512  276.745552\n",
       "tone        0.000276  0.071174  257.437722\n",
       "18          0.000276  0.056940  205.950178\n",
       "...              ...       ...         ...\n",
       "ill         0.049212  0.003559    0.072314\n",
       "da          0.029030  0.001779    0.061295\n",
       "later       0.030412  0.001779    0.058509\n",
       "lor         0.032347  0.001779    0.055008\n",
       "ltgt        0.056953  0.001779    0.031242\n",
       "\n",
       "[32651 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "tokens.sort_values(by='spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.23131672597864"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up spam_ratio for a given token\n",
    "tokens.loc['dating', 'spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Tuning the vectorizer\n",
    "\n",
    "Thus far, we have been using the default parameters of [CountVectorizer:](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show default parameters for CountVectorizer\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n",
    "\n",
    "- **stop_words**: string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range**: tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_df**: float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df**: float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Guidelines for tuning CountVectorizer**:\n",
    "\n",
    "    - Use your knowledge of the problem and the text, and your understanding of the tuning parameters, to help you decide what parameters to tune and how to tune them.\n",
    "    - Experiment, and let the data tell you the best approach!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
